version: "3.8"

services:
  zookeeper:
    image: wurstmeister/zookeeper:latest
    container_name: zookeeper
    ports:
      - "2181:2181"

  kafka:
    image: wurstmeister/kafka:latest
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181

      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9092,OUTSIDE://localhost:9093
      KAFKA_LISTENERS: INSIDE://0.0.0.0:9092,OUTSIDE://0.0.0.0:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE

      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"

      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "1"
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: "1"
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: "1"

      KAFKA_LOG_DIRS: /kafka/kafka-logs
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - kafka_data:/kafka/kafka-logs
    healthcheck:
      test: [ "CMD-SHELL", "/opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list >/dev/null 2>&1" ]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 20s

  kafdrop:
    image: obsidiandynamics/kafdrop
    container_name: kafdrop
    depends_on:
      - kafka
    ports:
      - "9000:9000"
    environment:
      KAFKA_BROKERCONNECT: kafka:9092

  ingest_coingecko:
    build:
      context: ./services/ingest_coingecko
    container_name: ingest-coingecko
    depends_on:
      kafka:
        condition: service_healthy
    env_file:
      - .env
    environment:
      KAFKA_BOOTSTRAP_SERVERS: "kafka:9092"
      KAFKA_TOPIC: "crypto_quotes_raw"
      POLL_SECONDS: "60"
      COINGECKO_BASE_URL: "https://api.coingecko.com/api/v3"
      VS_CURRENCY: "usd"
      ORDER: "market_cap_desc"
      PER_PAGE: "50"
      PAGE: "1"
      PRICE_CHANGE_PERCENTAGE: "24h"

  kafka-connect:
    build:
      context: ./services/connect
      dockerfile: Dockerfile
    container_name: kafka-connect
    depends_on:
      - kafka
    ports:
      - "8083:8083"
    environment:
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_BOOTSTRAP_SERVERS: "kafka:9092"
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: "connect-cluster"

      CONNECT_CONFIG_STORAGE_TOPIC: "_connect_configs"
      CONNECT_OFFSET_STORAGE_TOPIC: "_connect_offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "_connect_status"

      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"

      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"

      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"

      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"

      # AWS credentials are read from env vars by the connector (we'll configure it)
      AWS_REGION: "${AWS_REGION}"
      AWS_ACCESS_KEY_ID: "${AWS_ACCESS_KEY_ID}"
      AWS_SECRET_ACCESS_KEY: "${AWS_SECRET_ACCESS_KEY}"
      S3_BUCKET_NAME: "${S3_BUCKET_NAME}"
    restart: on-failure

  spark:
    build:
      context: ./services/spark
    container_name: spark
    env_file:
      - .env
    environment:
      AWS_ACCESS_KEY_ID: "${AWS_ACCESS_KEY_ID}"
      AWS_SECRET_ACCESS_KEY: "${AWS_SECRET_ACCESS_KEY}"
      AWS_REGION: "${AWS_REGION}"
    ports:
      - "8080:8080"   # Spark UI
      - "7077:7077"   # Spark master
    volumes:
      - ./services/spark_jobs:/opt/spark_jobs
    command: >
      bash -lc "/opt/spark/sbin/start-master.sh && tail -f /opt/spark/logs/*"

  spark-worker:
    build:
      context: ./services/spark
    container_name: spark-worker
    depends_on:
      - spark
    env_file:
      - .env
    environment:
      SPARK_MASTER_URL: spark://spark:7077
      AWS_ACCESS_KEY_ID: "${AWS_ACCESS_KEY_ID}"
      AWS_SECRET_ACCESS_KEY: "${AWS_SECRET_ACCESS_KEY}"
      AWS_REGION: "${AWS_REGION}"
    command: >
      bash -lc "/opt/spark/sbin/start-worker.sh spark://spark:7077 && tail -f /opt/spark/logs/*"

  spark-submit:
    build:
      context: ./services/spark
    container_name: spark-submit
    depends_on:
      - spark
      - spark-worker
    env_file:
      - .env
    user: "0:0"
    environment:
      AWS_ACCESS_KEY_ID: "${AWS_ACCESS_KEY_ID}"
      AWS_SECRET_ACCESS_KEY: "${AWS_SECRET_ACCESS_KEY}"
      AWS_REGION: "${AWS_REGION}"
      S3_BUCKET_NAME: "${S3_BUCKET_NAME}"
      BRONZE_PREFIX: "bronze/crypto_quotes_raw"
      SILVER_HISTORY_PREFIX: "silver/crypto_quotes_history"
      SILVER_LATEST_PREFIX: "silver/crypto_quotes_latest"
      SPARK_USER: spark
      SPARK_LOCAL_DIRS: /tmp/spark
      SPARK_SUBMIT_OPTS: "-Divy.cache.dir=/tmp/ivy -Divy.home=/tmp/ivy"

    volumes:
      - ./services/spark_jobs:/opt/spark_jobs
      - ivy_cache:/tmp/ivy

    entrypoint: [ "/opt/spark/bin/spark-submit" ]
    command:
      - "--master"
      - "spark://spark:7077"
      - "--deploy-mode"
      - "client"
      - "--packages"
      - "org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262"
      - "/opt/spark_jobs/crypto_silver/job.py"

volumes:
  kafka_data:
  connect_data:
  ivy_cache: